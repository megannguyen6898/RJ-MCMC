{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from math import exp\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "class logistic_regression:\n",
    "\n",
    "    def __init__(self, num_epochs, train_data, test_data, num_features, learn_rate, activation):\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        self.num_features = num_features\n",
    "        self.num_outputs = self.train_data.shape[1] - num_features\n",
    "        self.use_sigmoid = activation\n",
    "        self.w = np.random.uniform(-0.5, 0.5, num_features)\n",
    "        self.b = np.random.uniform(-0.5, 0.5, self.num_outputs)\n",
    "        self.learn_rate = learn_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.gradient = np.zeros(self.num_outputs)\n",
    "\n",
    "    def logisticactivtion(self, z_vec):\n",
    "        if self.use_sigmoid == True:\n",
    "            y = 1/(1 + np.exp(z_vec))\n",
    "        else:\n",
    "            y = z_vec\n",
    "        return y\n",
    "\n",
    "    def rmse(self, output, actual):\n",
    "        squared_mean = np.square(output - actual)//output.shape[0]\n",
    "        sum_sq_mean = np.sum(squared_mean)\n",
    "        return np.sqrt(sum_sq_mean)\n",
    "\n",
    "    def output(self, x_vec):\n",
    "        z = x_vec.dot(self.w)-self.b\n",
    "        output = self.logisticactivtion(z)\n",
    "        return output\n",
    "\n",
    "    def gradient(self, output, actual): #derivative of output w.r.t x\n",
    "        if self.use_sigmoid == True:\n",
    "            gradient = (output - actual) * output * (1 - output) # there are only two discrete outcomes (1 or 0), this is a Bernoulli distribution\n",
    "        else: #if not use sigmoid then linear\n",
    "            gradient = output - actual\n",
    "        return gradient\n",
    "\n",
    "    def update(self, gradient, x_vec):\n",
    "        self.w += self.learn_rate * gradient * x_vec\n",
    "        self.b += self.learn_rate * gradient * 1\n",
    "\n",
    "    def encoder(self, w): #get parameters and put them in fixed-shape model\n",
    "        self.w = w[0:self.num_features]\n",
    "        self.b = w[self.num_features:] #get b as the rest of parameters\n",
    "\n",
    "    def predicted_output(self, data, w):\n",
    "        self.encoder(w)\n",
    "        fx = np.zeros(data.shape[0])\n",
    "        for i in range(0, data.shape[0]):\n",
    "            input = data[i, 0:self.num_features] #i - which rows to get and 0:self.num_features - which columns to get\n",
    "            predicted_output = self.output(input)\n",
    "            fx[i] = predicted_output\n",
    "        return fx\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "class MCMC:\n",
    "    \n",
    "    def __init__(self,topology):\n",
    "        self.topology = topology #[Weights, Bias]\n",
    "\n",
    "    def rmse(self, output, actual):\n",
    "        squared_mean = np.square(output - actual)//output.shape[0]\n",
    "        sum_sq_mean = np.sum(squared_mean)\n",
    "        return np.sqrt(sum_sq_mean)\n",
    "\n",
    "    def likelihood_function(self, model, data, tausq, w):\n",
    "        #tausq is the extent of variation among the effects observed in different studies \n",
    "        y = data[:, self.topology[0]] #collect all columns of weights that act as inputs\n",
    "        fx = model.predicted_output(data, w)\n",
    "        accuracy = self.rmse(fx, y)\n",
    "        loss = (-1/2) * np.log(2 * math.pi * tausq) - 1/2 * np.square(y - fx) / tausq\n",
    "            \n",
    "        return [np.sum(loss), fx, accuracy]\n",
    "\n",
    "    def prior_normal(self, w, sigmasq, nu_1, nu_2, tausq):\n",
    "        num_w = self.topology[0] + 1 #number of weights\n",
    "        #find log of the PDF (or maximum likelihood estimation) of normal distribution, our prior distribution\n",
    "        log_mle = (-1/2 * num_w * np.log(sigmasq)) - (1/2 * np.sum(np.square(w)) / sigmasq) - ((1 + nu_1) * np.log(tausq)) - (nu_2 / tausq)\n",
    "        return log_mle\n",
    "\n",
    "    def sampler(self):\n",
    "        samples = self.samples\n",
    "        ##split data into train and test set\n",
    "        data_train, data_test = train_test_split(self.data, test_size=0.25)\n",
    "        train_size = data_train.shape[0]\n",
    "        test_size = data_test.shape[0]\n",
    "\n",
    "        ##Initialize train and test set\n",
    "        X_train = np.linspace(0, 1, train_size)\n",
    "        X_test = np.linspace(0, 1, test_size)\n",
    "\n",
    "        y_train = data_train[:, self.topology[0]]\n",
    "        y_test = data_test[:, self.topology[0]]\n",
    "\n",
    "        #number of weights and bias\n",
    "        w_size = self.topology[1] + self.topology[0]\n",
    "\n",
    "        ##Initialize fx of train and test data over all samples\n",
    "        fx_train = np.ones((samples, train_size)) #array with row-columns as samples-train_size\n",
    "        fx_test = np.ones((samples, test_size))\n",
    "        rmse_train = np.zeros(samples)\n",
    "        rmse_test = np.zeros(samples)\n",
    "\n",
    "        ##Initialize posterior distribution of weights over all samples \n",
    "        pos_w = np.ones((samples, w_size)) # values of all weights and bias drawn from posterior distribution over all samples\n",
    "        pos_tau = np.ones((samples, 1))\n",
    "\n",
    "        ##Propose initial distribution for w\n",
    "        w = np.random.randn(w_size) #return a sample of w from standard normal distribution \n",
    "        proposed_w = np.random.randn(w_size) #proposed value of w\n",
    "        step_w = 0.02  # defines how much variation you need in changes to w\n",
    "        step_eta = 0.01 # eta is an additional parameter to cater for noise in predictions (Gaussian likelihood)\n",
    "\n",
    "        ##Evaluate initial w (at epoch 0)\n",
    "        #Model\n",
    "        model = logistic_regression(0, data_train, data_test, self.topology[0], 0.1, self.regression)\n",
    "\n",
    "        predicted_train_output = model.predicted_output(data_train, w)\n",
    "        predicted_test_output = model.predicted_output(data_test, w)\n",
    "        eta = np.log(np.var(y_train - predicted_train_output))\n",
    "        proposed_tau = np.exp(eta)\n",
    "\n",
    "        ##Prior Distribution\n",
    "        #sigmasq_nu = {'sigmasq': 5, 'nu_1': 0, 'nu_2':0} #Create a dict to avoid the problem positional argument (proposed_tau) can't appear after keyword arg (sigmasq=5,..)\n",
    "        sigmasq = 5\n",
    "        nu_1 = 0\n",
    "        nu_2 = 0\n",
    "        prior = self.prior_normal(w, sigmasq, nu_1, nu_2, proposed_tau)\n",
    "\n",
    "        ##Likelihood\n",
    "        [likelihood, fx_train, rmsetrain] = self.likelihood_function(model, data_train, proposed_tau, w)\n",
    "        print('Initial Likelihood is:', likelihood)\n",
    "\n",
    "        ##Propose the moves\n",
    "        naccept = 0\n",
    "        for i in range(samples-1):\n",
    "            proposed_w = w + np.random.normal(0, step_w, w_size)\n",
    "            proposed_eta = eta + np.random.normal(0, step_eta, 1)\n",
    "            proposed_tau = np.exp(proposed_eta)\n",
    "\n",
    "            #Find likelihood and prior for proposed w\n",
    "            prior_proposal = self.prior_normal(proposed_w, sigmasq, nu_1, nu_2, proposed_tau)\n",
    "            [likelihood_proposal, fx_train_propose, rmsetrain_propose] = self.likelihood_function(model, data_train, proposed_tau, proposed_w)\n",
    "\n",
    "            #Find the probability of transitioning from state w to proposed w, which has a unique stationary distribution Ï€(w)\n",
    "            prior_diff = prior_proposal - prior\n",
    "            likelihood_diff = likelihood_proposal - likelihood\n",
    "\n",
    "            accept_prob = np.min(1, np.exp(prior_diff + likelihood_diff)) #because both values in log form\n",
    "            #Generate a random uniform u\n",
    "            u = random.uniform(0, 1)\n",
    "\n",
    "            if u < accept_prob or u == accept_prob:\n",
    "                naccept += 1\n",
    "                likelihood = likelihood_proposal\n",
    "                prior = prior_proposal\n",
    "                eta = proposed_eta\n",
    "                w = proposed_w\n",
    "\n",
    "                print(likelihood, prior, w, rmse_train, 'accepted')\n",
    "\n",
    "                #Update posterior distribution\n",
    "                pos_w[i+1, ] = proposed_w #means there is only 1 column or 1-D arra\n",
    "                pos_tau[i+1, ] = proposed_tau\n",
    "                fx_train[i+1, ] = fx_train_propose\n",
    "                rmse_train[i+1, ] = rmsetrain_propose\n",
    "\n",
    "            else: #Keep posterior distribution (!!!!) intact\n",
    "                pos_w[i+1, ] = pos_w[i, ]\n",
    "                pos_tau[i+1, ] = pos_tau[i, ]\n",
    "                fx_train[i+1, ] = fx_train[i, ]\n",
    "                rmse_train[i+1, ] = rmse_train[i, ]\n",
    "\n",
    "        accept_ratio = 100 * (naccept / samples)\n",
    "        print(accept_ratio, '% was accepted')\n",
    "\n",
    "        ##Use burnin\n",
    "        #Start at x, then run the Markov chain for n steps, from which throw away all the data (no output). n is the burn-in period. After the burn-in run normally, using each iterate in MCMC calculations\n",
    "        burnin = 0.25 * samples  # use post burn in samples\n",
    "\n",
    "        pos_w = pos_w[int(burnin):, ]\n",
    "        pos_tau = pos_tau[int(burnin):, ] \n",
    "        fx_train = fx_train[int(burnin):, ]\n",
    "        rmse_train = rmse_train[int(burnin):]\n",
    "\n",
    "        ##Test Bayesian model via posterior distributions of a certain # of samples over n trials\n",
    "        num_trials = 100\n",
    "\n",
    "        accuracy_posterior =np.zeros(num_trials)\n",
    "        for i in range(num_trials):\n",
    "            w_drawn = np.random.normal(pos_w.mean(axis=0), pos_w.std(axis=0), w_size) #draw random samples of w from normal distribution with parameters: mean and std of each value of vector 'posterior w' over the certain # of samples\n",
    "            #axis=0 means calculated mean across the rows\n",
    "            tau_drawn = np.random.normal(pos_tau.mean, pos_tau.std)\n",
    "            #Check loss and accuracy of predictions made with posterior weights    \n",
    "            [loss_posterior, fx_posterior, accuracy_posterior[i]] = self.likelihood_function(model, data_train, tau_drawn, w_drawn)\n",
    "            print('Testing posterior distributions:', i, loss_posterior, accuracy_posterior[i], tau_drawn, pos_tau.mean(), pos_tau.std())\n",
    "            print('Mean and std of accuracy of posterior test:', accuracy_posterior.mean(), accuracy_posterior.std())\n",
    "\n",
    "        return (pos_w, pos_tau, fx_train, rmse_train, accept_ratio)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.12 64-bit ('env_pytorch': conda)"
  },
  "interpreter": {
   "hash": "164dfdaf806f60fca5f093b6a424355790f856c9afd16dfa99ca3711d938d3e3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}